#!/usr/bin/env python

# Using the input XML file from the official L2 Aggregator, create a mapping of dataset names
# coming out of the L2 full physics files to what they would be in the official aggregated files.
# Requires as argument the following files from the L2AggPGE repo:
# * oco2_L2Agg_apf_140820143657.xml        
# * oco2_L2Dia_OutHDFDef_140825213246.xml
# Additionally the xsd file is needed:
# * oco_parameterfile_060401120000.xsd
# 
# L2AggPGE repo:
# https://svn.jpl.nasa.gov/trac/browser/sdos/oco2/trunk/PGE/L2AggPGE/tables

from __future__ import print_function
import os
import sys
from argparse import ArgumentParser

from pprint import pprint
import xml.etree.ElementTree as ET

parser = ArgumentParser()

parser.add_argument("--apf", dest='apf_list',  nargs='+')
parser.add_argument("--def", dest='hdf_def_list',  nargs='+')

args = parser.parse_args()

mapping = {}
agg_dataset_names = set()

# Add non L2AggPGE mappings only output values
mapping['tropopause_altitude'] = {'name': 'RetrievalResults/tropopause_altitude', 'shape': 'Retrieval_Array'}
mapping['tropopause_pressure'] = {'name': 'RetrievalResults/tropopause_pressure', 'shape': 'Retrieval_Array'}
mapping['vector_altitude_levels'] = {'name': 'RetrievalResults/vector_altitude_levels', 'shape': 'Retrieval_Level_Array'}

for ds_name in [ mapping[m]['name'] for m in mapping.keys() ]:
    agg_dataset_names.add(ds_name)

# Come up with which apf data to search from
search_groups = [
        "ScienceH5FileStateVectorFields",
        "ScienceH5FileRetrievalFields", 
        "SubsettedL1BFieldNames",
        "SubsettedL1BFrameFieldNames",
        "SubsettedL1BFieldNames",
        "SubsettedDOASFieldNames",
        "SubsettedCloudFieldNames",
        "SubsettedSndSelFieldNames",
        ]

for apf_fn, hdf_def_fn in zip(args.apf_list, args.hdf_def_list):
    apf_file = ET.parse(apf_fn)
    hdf_def_file = ET.parse(hdf_def_fn)

    search_fields = []
    for grp_name in search_groups:
        srch1 = apf_file.find("*[@name='%s']" % grp_name)
        if srch1 != None:
            search_fields += srch1
        else:
            srch2 = apf_file.find("*/*[@name='%s']" % grp_name)

            if srch2 != None:
                search_fields += srch2
            else:
                print("Could not find group: %s in %s" % (grp_name, apf_fn), file=sys.stderr)

    # Figure out mapping from HDF XML file
    for field in search_fields:
        src_name = field.text

        # Skip existing mappings
        if src_name in mapping.keys():
            continue

        dst_name = field.attrib["name"]

        field_info =  hdf_def_file.find("*/*[@name='%s']" % dst_name)
        dst_shape = field_info.find("*[@name='Shape']").text

        field_group = hdf_def_file.find("*/*[@name='%s'].." % dst_name)
        group_name = field_group.attrib["name"]


        mapping[src_name] = { "name": "%s/%s" % (group_name, dst_name), "shape": dst_shape }

    # Get list of all dataset names known by the aggregator
    for group_elem in hdf_def_file.findall('group'):
        group_name = group_elem.attrib['name']
        for dataset_elem in group_elem.findall('group'):
            ds_name = dataset_elem.attrib['name']
            agg_dataset_names.add( '%s/%s' % (group_name, ds_name))

print("# Generated by: %s" % os.path.basename(sys.argv[0]))
print("# From: %s, %s" % (', '.join([ os.path.basename(a) for a in args.apf_list ]), ', '.join([ os.path.basename(h) for h in args.hdf_def_list ])))
print("aggregator_dataset_mapping = \\")
pprint(mapping)
print("")
print("aggregator_dataset_dest_names = \\")
pprint(sorted(agg_dataset_names))
